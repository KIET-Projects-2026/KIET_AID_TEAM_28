ğŸŒ From Scratch Translation Models (English â†” Telugu)
TEAM 28 â€“ KIET AID Project
ğŸ“Œ Project Overview

This project focuses on building and analyzing neural machine translation (NMT) systems for English â†” Telugu translation.

We implement:

A Transformer-based encoderâ€“decoder model from scratch

A full data pipeline (cleaning, tokenization, training, evaluation)

A web application with:

Frontend: Vite + React

Backend: Flask (Python)

Comparative analysis against state-of-the-art pre-trained models:

Google FLAN

Facebook NLLB

The project emphasizes learning fundamentals, experimental rigor, and reproducibility.

ğŸ¯ Objectives & Deliverables
Goals

Build a Transformer model from first principles

Train and evaluate it on Englishâ€“Telugu parallel data

Compare performance with strong pre-trained baselines

Provide a usable web interface for translation

Deliverables

Cleaned & tokenized datasets

Transformer architecture implementation

Training & evaluation pipeline

Metrics: BLEU, sacreBLEU, chrF

Error analysis & ablation studies

Web-based translation demo

Reproducible GitHub repository

ğŸ§  Technologies Used (Tech Stack)
Frontend

âš¡ Vite

âš›ï¸ React.js

HTML, CSS, JavaScript

REST API integration

Backend

ğŸ Flask (Python)

RESTful API for translation requests

Model inference endpoints

Machine Learning / NLP

ğŸ¤– Neural Networks

ğŸ§© Transformer (Encoderâ€“Decoder)

ğŸ”¤ SentencePiece (BPE / Unigram)

ğŸ”¥ PyTorch

ğŸ§ª Evaluation: BLEU, sacreBLEU, chrF

Pre-trained Baselines

ğŸŸ¢ Google FLAN

ğŸ”µ Facebook NLLB (No Language Left Behind)

ğŸ¤— Hugging Face Transformers

Tools & Utilities

Git & GitHub

Git LFS (for large files)

TensorBoard / Weights & Biases

Python (NumPy, pandas)

ğŸ“Š Model Approaches
1ï¸âƒ£ From-Scratch Transformer

Encoderâ€“Decoder architecture

Multi-Head Attention

Positional Encoding

Teacher forcing with cross-entropy loss

Trained only on parallel data

2ï¸âƒ£ Pre-trained Baselines

Google FLAN: Instruction-tuned large language model

Facebook NLLB: Multilingual translation-focused model

Used for benchmarking & comparison

ğŸ“‚ Project Structure
project-root/
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ tokenized/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ eval/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ frontend/                  # Vite + React app
â”œâ”€â”€ backend/                   # Flask server
â”œâ”€â”€ experiments/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ checkpoints/               # ignored via git / tracked via LFS
â”œâ”€â”€ logs/
â”œâ”€â”€ docs/
â””â”€â”€ scripts/

ğŸš€ How to Run the Project
Backend (Flask)
cd backend
pip install -r requirements.txt
python app.py


Runs on:

http://127.0.0.1:5000

Frontend (Vite + React)
cd frontend
npm install
npm run dev


Runs on:

http://localhost:5173

ğŸ“ˆ Evaluation Metrics

BLEU / sacreBLEU â€“ Standard MT quality

chrF â€“ Character-level metric (better for Telugu)

Human qualitative error analysis

ğŸ§ª Experiments Conducted

Small vs Standard Transformer

Shared vs Separate vocabularies

Label smoothing on/off

Dropout variations

From-scratch vs Pre-trained models

Ablation studies (positional encoding, attention heads)

âš™ï¸ Practical Considerations

GPU recommended (16GB+ preferred)

Mixed precision (FP16) supported

Gradient accumulation for low-resource setups

Reproducibility via fixed seeds & configs

ğŸ“š Datasets Used

Samanantar

OPUS / Tatoeba

FLORES-101 (Evaluation)

Dataset licenses are respected and documented.
